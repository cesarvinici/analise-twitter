---
title: "Insights Twitter"
author: "Cesar Vinicius <cesarvinici@gmail.com>"
date: "24/11/2019"
output:
  html_document: default
  pdf_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(tidy = FALSE, tidy.opts = list(comment = FALSE))
```
```{r fig.keep='all', message=FALSE, warning=FALSE, include=FALSE, paged.print=FALSE, results='hide'}
knitr::opts_chunk$set(tidy = TRUE, tidy.opts = list(comment = FALSE))
setwd("~/Documents/analise-twitter")

#install.packages("mongolite")
#install.packages("abjutils")
#install.packages("wordcloud")
#install.packages("wesanderson") # Paleta de cores
#install.packages("RColorBrewer")
# install.packages("rmarkdown")


library(mongolite)
library(stringr)
library(tibble)
library(tm)
library(abjutils)
library(tidytext)
library(dplyr)
library(readr)
library(wordcloud)
library(wesanderson)
library(tidyr)
library(ggplot2)
library(igraph)
library(ggraph)
library(RColorBrewer)

# Conecting to mongoDB
m <- mongo(url = "mongodb://172.18.0.3/", options = ssl_options(weak_cert_validation = T), db="twitter", collection="tweets_lula")

#Getting tweets from database and saving on R variable
tweets <- m$find(
  fields = '{"_id": false, "text": true}',
)

# stop words
stop_words <- read_delim("stopwords.csv", ";", escape_double = FALSE, trim_ws = TRUE)
stop_words <- add_row(stop_words, word = "to")
stop_words <- add_row(stop_words, word = "tô")
stop_words <- add_row(stop_words, word = "ta")
stop_words <- add_row(stop_words, word = "tá")
stop_words <- add_row(stop_words, word = "q")
stop_words <- add_row(stop_words, word = "pq")
stop_words <- add_row(stop_words, word = "ah")
stop_words <- add_row(stop_words, word = "ai")
stop_words <- add_row(stop_words, word = "s")
stop_words <- add_row(stop_words, word = "x")
stop_words <- add_row(stop_words, word = "tt")
stop_words <- add_row(stop_words, word = "tto")
stop_words <- add_row(stop_words, word = "tts")
stop_words <- add_row(stop_words, word = "vc")
stop_words <- add_row(stop_words, word = "2155")
stop_words <- add_row(stop_words, word = "msm")
stop_words <- add_row(stop_words, word = "mto")
stop_words <- add_row(stop_words, word = "oq")
stop_words <- add_row(stop_words, word = "p")
stop_words <- add_row(stop_words, word = "811")
stop_words <- add_row(stop_words, word = "vai")
stop_words <- add_row(stop_words, word = "vem")
stop_words <- add_row(stop_words, word = "sei")
stop_words <- add_row(stop_words, word = "sai")
stop_words <- add_row(stop_words, word = "fala")
stop_words <- add_row(stop_words, word = "fez")
stop_words <- add_row(stop_words, word = "pode")
stop_words <- add_row(stop_words, word = "agr")
stop_words <- add_row(stop_words, word = "falar")
stop_words <- add_row(stop_words, word = "não")
stop_words <- add_row(stop_words, word = "vão")
stop_words <- add_row(stop_words, word = "vou")
stop_words <- add_row(stop_words, word = "dois")
stop_words <- add_row(stop_words, word = "quer")
stop_words <- add_row(stop_words, word = "né")
stop_words <- add_row(stop_words, word = "ver")
stop_words <- add_row(stop_words, word = "viu")
stop_words <- add_row(stop_words, word = "nao")
stop_words <- add_row(stop_words, word = "leon")
stop_words <- add_row(stop_words, word = "K k")
stop_words <- add_row(stop_words, word = "dia")
stop_words <- add_row(stop_words, word = "ficar")
stop_words <- add_row(stop_words, word = "quero")
stop_words <- add_row(stop_words, word = "hj")
stop_words <- add_row(stop_words, word = "faz")
stop_words <- add_row(stop_words, word = "coisa")
stop_words <- add_row(stop_words, word = "dia")
stop_words <- add_row(stop_words, word = "fazer")
stop_words <- add_row(stop_words, word = "perfil")
stop_words <- add_row(stop_words, word = "cara")
stop_words <- add_row(stop_words, word = "acho")
stop_words <- add_row(stop_words, word = "bom")
stop_words <- add_row(stop_words, word = "c")
stop_words <- add_row(stop_words, word = "dar")
stop_words <- add_row(stop_words, word = "k")
stop_words <- add_row(stop_words, word = "eh")
stop_words <- add_row(stop_words, word = "vcs")
stop_words <- add_row(stop_words, word = "cu")
stop_words <- add_row(stop_words, word = "fica")
stop_words <- add_row(stop_words, word = "deve")
stop_words <- add_row(stop_words, word = "falando")
stop_words <- add_row(stop_words, word = "diz")
stop_words <- add_row(stop_words, word = "tomar")
stop_words <- add_row(stop_words, word = "lado")
stop_words <- add_row(stop_words, word = "querem")
stop_words <- add_row(stop_words, word = "sendo")
stop_words <- add_row(stop_words, word = "dizer")
stop_words <- add_row(stop_words, word = "maior")
stop_words <- add_row(stop_words, word = "saber")
stop_words <- add_row(stop_words, word = "favor")
stop_words <- add_row(stop_words, word = "vamos")
stop_words <- add_row(stop_words, word = "sabe")
stop_words <- add_row(stop_words, word = "tempo")
stop_words <- add_row(stop_words, word = "via")
stop_words <- add_row(stop_words, word = "pau")
stop_words <- add_row(stop_words, word = "gente")
stop_words <- add_row(stop_words, word = "ir")
stop_words <- add_row(stop_words, word = "olha")
stop_words <- add_row(stop_words, word = "ia")
stop_words <- add_row(stop_words, word = "coisas")
stop_words <- add_row(stop_words, word = "visto")
stop_words <- add_row(stop_words, word = "dá")
stop_words <- add_row(stop_words, word = "so")
stop_words <- add_row(stop_words, word = "vi")
stop_words <- add_row(stop_words, word = "fico")
stop_words <- add_row(stop_words, word = "tava")
stop_words <- add_row(stop_words, word = "sair")
stop_words <- add_row(stop_words, word = "ja")
stop_words <- add_row(stop_words, word = "deu")

affin_pt <- read_delim("affin_pt.csv",
                       ";",escape_double = FALSE, trim_ws = TRUE)


# Removing urls
patternRegexUrl <- 'https?:\\/\\/(www\\.)?[-a-zA-Z0-9@:%._\\+~#=]{1,256}\\.[a-zA-Z0-9()]{1,6}\\b([-a-zA-Z0-9()@:%_\\+.~#?&//=]*)'
tweets$text <- str_replace(tweets$text, patternRegexUrl, "") 
patternRegexKs <- 'k{2,}'
tweets$text <- str_replace(tweets$text, patternRegexKs, "")

# Removing @users reference from text
tweets$text <- str_replace_all(tweets$text,  "@\\w+", "") %>%
              removePunctuation() %>% # REMOVE PONTUAÇÃO
              stripWhitespace() # REMOVE ESPAÇOS EM BRANCO E PALAVRAS ACENTUADAS

# Removing emojis
# tweets$text <- gsub("[^\x01-\x7F]", "", tweets$text)

# SEPARA POR PARLAVRA E REMOVE AS STOPWORDS
tokens_tweets <- tweets %>%
  mutate(linenumber = row_number()) %>%
  unnest_tokens(word, text) %>%
  anti_join(stop_words)

tokens_tweets$word = str_replace_all(tokens_tweets$word, " ", "")
  
# CONTA AS PALAVRAS
tokens_tweets_count <- tokens_tweets %>%
   filter(word != "lula") %>%
  count(word, sort = TRUE)

#Aumenta a paleta de cores
nb.cols <- 20
mycolors <- colorRampPalette(brewer.pal(8, "Set2"))(nb.cols)
```

### Analise de mensagens no twitter no dia 08/11/2019

Vamos (tentar) analisar alguns tweets gerados no dia 08/11/2019 e ver qual insights podemos tirar de lá.

Antes que me perguntem, **porque dia 08/11/2019?**
  
- Bom, imaginei que usuários de do twitter estariam pavorosos pois foi o dia que o ex presidente Lula foi solto da prisão após a decisao sobre prisões em segunda instância do STF.

Isso dito acredito que um *disclaimer* seja necessário. Bem sabemos que política é um assunto um tanto perigoso para se tratar no Brasil nos últimos tempos, pois de modo geral as pessoas estão cada vez mais polarizadas. Com isso minha intenção aqui é ser o mais imparcial possível.

Três pesquisas foram realizadas no twitter neste dia, usei as *keywords* **Lula** e **Bolsonaro** e uma terceira pesquisa sem uma *keyword* definida, de forma a pegar tudo possível.

Como uso uma versão gratuita da API do twitter, a pesquisa foi um tanto limitada, consegui apenas 4/5 mil tweets em cada pesquisa. Mas acredito que já de pra tirar uns *insights* interessantes!


Para realizar este trabalho foi usado Python para coleta e salvamento dos dados em um container docker rodando um MongoDB e R para as analises.

Todo código está disponível no [Github](https://github.com/cesarvinici)

### Lula

Vamos primeiro para análise utilizando a *keyword* 'Lula':

- Abaixo uma *WordCloud* com as palavras que mais aparecem na pesquisa: 

```{r pressure, echo=FALSE, fig.cap="WordCloud Keyword Lula", out.width = '85%'}
wordcloud(tokens_tweets_count$word, 
          tokens_tweets_count$n, 
          max.words =50, 
          scale = c(8,.5),
          rot.per=0.1,
          color=mycolors)
```

*ps*: Como a pesquisa no twitter foi realizada usando a palavra-chave "Lula", não faz sentido plotar ela, já que ela vai aparecer em todos os 4.5 mil tweets.

\s Algumas palavras já imaginavámos que iriam aparecer, como "Livre" de (Lula Livre) e afins. Percebemos que a palavra 'Bolsonaro' também aparece com uma boa frequência. É possível verificar que algumas pessoas não ficaram muito feliz com a decisão pois as palavras "ladrão" e "bandido" aparecem. Algumas provocações foram feitas para eleitores do Bolsonaro, já que a palavra comumente usada "Bolsominion" também aparece com certa frequência.

Para confirmar algumas previsões, vamos ver um gráfico de barras mostrando a contagem das palavras:

```{r echo=FALSE, fig.cap="WordCloud Keyword Lula", out.width = '90%'}
# Plotando palavras mais usadas
tokens_tweets_count %>%
  mutate(word = reorder(word,n)) %>%
  head(10) %>%
  ggplot(aes(word,n,fill=factor(word)))+
  scale_fill_manual(values = mycolors)+
  geom_col()+
   xlab("Palavras")+
  ylab("Quantidade") +
    coord_flip()
```

Nenhuma surpresa quando as palavras "livre" (Lula livre) ou solto, Bolsonaro está no top 3 o que pode ser surpreendente e ao mesmo tempo levar à indagação se são eleitores do Bolsonaro ou eleitores do Lula que estão preocupados com o canditado do outro lado do polo?

Beleza, já sabemos quais foram as palavras que mais se repitiram, mas e as frases? Bigramas, Trigramas e etc? é possível saber? 
\s ***yes it is!**

Vamos aos bigramas (se você não sabe o que é um bigrama [Clique aqui](https://en.wikipedia.org/wiki/Bigram))

```{r echo=FALSE}
# CRIANDO UM BIGRAMA
bigrams <- tweets %>%
  unnest_tokens(bigram, text, token = "ngrams", n = 2) %>% # Separa as palavras de 2 em 2
  separate(bigram,c("word1","word2"),sep = " ") %>% # Divide entre palavra 1 e palavra 2
  filter(!word1 %in% as.vector(t(stop_words$word))) %>%
  filter(!word2 %in% as.vector(t(stop_words$word))) %>% # Filtra as palavras a partir das stop_words
  unite(bigram,word1,word2,sep = " ") %>%
  count(bigram, sort = TRUE)

bigrams %>%
  mutate(bigram = reorder(bigram,n)) %>%
  head(20) %>%
  ggplot(aes(bigram,n,fill=factor(bigram)))+
  scale_fill_manual(values = mycolors)+
  geom_col()+
  ylab("Quantidade")+
  xlab("Bigrama") +
  coord_flip()
```

Bom, pra surpresa de ninguém o bigrama mais falado foi "Lula livre", e de modo geral com excessão de Boa noite e jornal nacional todas as outras frases foram em um sentido de comemoração à solturado Ex Presidente.

E frases com 3 ou mais palavras? 

```{r echo=FALSE}
trigrams <- tweets %>%
  unnest_tokens(trigrams, text, token = "ngrams", n = 3) %>% # Separa as palavras de 2 em 2
  separate(trigrams,c("word1","word2", "word3"),sep = " ") %>% # Divide entre palavra 1 e palavra 2
  filter(!word1 %in% as.vector(t(stop_words$word))) %>%
  #filter(!word2 %in% as.vector(t(stopwords_pt$word))) %>% # Filtra as palavras a partir das stop_words
  filter(!word3 %in% as.vector(t(stop_words$word))) %>%
  unite(trigrams,word1,word2, word3,sep = " ") %>%
  count(trigrams, sort = TRUE)

trigrams %>%
  mutate(trigrams = reorder(trigrams,n)) %>%
  head(9) %>%
  ggplot(aes(trigrams,n,fill=factor(trigrams)))+
  scale_fill_manual(values = mycolors) +
  geom_col()+
  coord_flip()

# CREATE TRIGAMS
quadrams <- tweets %>%
  unnest_tokens(quadrams, text, token = "ngrams", n = 4) %>% # Separa as palavras de 2 em 2
  separate(quadrams,c("word1","word2", "word3", "word4"),sep = " ") %>% # Divide entre palavra 1 e palavra 2
  filter(!word1 %in% as.vector(t(stop_words$word))) %>%
  #filter(!word2 %in% as.vector(t(stopwords_pt$word))) %>% # Filtra as palavras a partir das stop_words
  filter(!word4 %in% as.vector(t(stop_words$word))) %>%
  unite(quadrams,word1,word2, word3, word4,sep = " ") %>%
  count(quadrams, sort = TRUE)

quadrams %>%
  mutate(quadrams = reorder(quadrams,n)) %>%
  head(9) %>%
  ggplot(aes(quadrams,n,fill=factor(quadrams)))+
  scale_fill_manual(values = mycolors) +
  geom_col()+
  coord_flip()
```

Via de regra são pessoas ou comemorando ou, ao que parece simplemente comentando a soltura.

Bom, se formos analisar friamente não obtivemos nenhuma grande surpresa nas palavras nem nas frases ditas pelos usuários do Twitter ao que se refere ao Lula, mas e em uma analise de sentimentos, será que eles se portaram de forma mais agressiva ou mais positiva? 

Vamos analisar! 

```{r echo=FALSE, results="markup"}
# Analise de Sentimentos (verifica a diferença entre palavras negaticas e positivas)
  affin <- tokens_tweets %>%
  inner_join(affin_pt) %>%
  count(index=linenumber %/% 25, sentiment) %>%
  spread(sentiment,n,fill=0) %>%
  mutate(sentiment = positivo - negativo)

positivo <- sum(affin['positivo'])
negativo <- sum(affin['negativo'])

sentimentos <- c("Positivo", "Negativo")
sentimentos_count <- c(sum(affin['positivo']), sum(affin['negativo']))
dados <- data.frame(sentimentos, sentimentos_count, stringsAsFactors = FALSE)

ggplot(data = dados, aes(x = sentimentos)) +
  ylab("Qtd. Palavras") +
  geom_col(aes(y = sentimentos_count), position = "dodge",fill = "grey50", colour = "black")

 
```


Na comparação palavras com um tom mais negativos foram mais "faladas" que as positivas, porém precisamos levar em consideração o contexto, já que "palavrões" foram usados em um tom de positivo, de comemoração.


### Bolsonaro
```{r fig.keep='all', message=FALSE, warning=FALSE, include=FALSE, paged.print=FALSE, results='hide'}
knitr::opts_chunk$set(tidy = TRUE, tidy.opts = list(comment = FALSE))
m <- mongo(url = "mongodb://172.18.0.3/", options = ssl_options(weak_cert_validation = T), db="twitter", collection="tweets_bolsonaro")

#Getting tweets from database and saving on R variable
tweets <- m$find(
  fields = '{"_id": false, "text": true}',
)

patternRegexUrl <- 'https?:\\/\\/(www\\.)?[-a-zA-Z0-9@:%._\\+~#=]{1,256}\\.[a-zA-Z0-9()]{1,6}\\b([-a-zA-Z0-9()@:%_\\+.~#?&//=]*)'
tweets$text <- str_replace(tweets$text, patternRegexUrl, "") 
patternRegexKs <- 'k{2,}'
tweets$text <- str_replace(tweets$text, patternRegexKs, "")


# Removing @users reference from text
tweets$text <- str_replace_all(tweets$text,  "@\\w+", "") %>%
              removePunctuation() %>% # REMOVE PONTUAÇÃO
              stripWhitespace() # REMOVE ESPAÇOS EM BRANCO E PALAVRAS ACENTUADAS

# Removing emojis
# tweets$text <- gsub("[^\x01-\x7F]", "", tweets$text)

# SEPARA POR PARLAVRA E REMOVE AS STOPWORDS
tokens_tweets <- tweets %>%
  mutate(linenumber = row_number()) %>%
  unnest_tokens(word, text) %>%
  anti_join(stop_words)

tokens_tweets$word = str_replace_all(tokens_tweets$word, " ", "")
  
# CONTA AS PALAVRAS
tokens_tweets_count <- tokens_tweets %>%
  filter(word != "bolsonaro") %>%
  count(word, sort = TRUE)

#Aumenta a paleta de cores
nb.cols <- 20
mycolors <- colorRampPalette(brewer.pal(8, "Set2"))(nb.cols)
```
Vamos para a pesquisa utilizando a palavra-chave "Bolsonaro"

*ps*: Da mesma forma que tiramos a palavra-chave Lula, aqui também iremos tirar Bolsonaro, já que, ela aparece em todos os twites.

```{r echo=FALSE}
wordcloud(tokens_tweets_count$word, 
          tokens_tweets_count$n, 
          max.words =50, 
          scale = c(4,.5),
          rot.per=0.1,
          color=mycolors)
```

Aqui a palavra mais usada foi, Lula. Ou seja, para algumas pessoas é quase impossível falar de um sem falar do outro. Também temos presidente, e Jair, o que é meio óbvio. A palavra filho também aparece na lista, provavelmente direcionada a algum filho politico do Bolsonaro. Percebe-se também que a galera já está preocupada com as eleições de 2022. Algumas pessoas estão brabos com a soltura. o Ex-juiz Sergio Moro também foi bastante citado ao que parece, será que pro bem ou pro mal? Fica da dúvida.

Bom, podemos comprovar algumas teorias olhando a contagem de palavras, vamos lá!

```{r echo=FALSE}
# Plotando palavras mais usadas
tokens_tweets_count %>%
  mutate(word = reorder(word,n)) %>%
  head(20) %>%
  ggplot(aes(word,n,fill=factor(word)))+
  scale_fill_manual(values = mycolors)+
  geom_col()+
  xlab("Palavras")+
  ylab("Quantidade") +
  coord_flip()
```

Não se enganem, a palavra-chave ainda é Bolsonaro. Por mais que olhando para o gráfico, existem muitas referências ao PT ou ao Lula. Foram recuperados 3900 twites com a palavra-chave bolsonaro, e em mais de 1500 também aparece a palavra Lula. Lembrando que quando fizemos utilizando a palavra-chave Lula, o qual retornou mais de 4 mil twites, Bolsonaro não chegou a ser citado 500 vezes.

Bigramas, Trigramas e Quadrigramas.

```{r echo=FALSE}


# CRIANDO UM BIGRAMA
bigrams <- tweets %>%
  unnest_tokens(bigram, text, token = "ngrams", n = 2) %>% # Separa as palavras de 2 em 2
  separate(bigram,c("word1","word2"),sep = " ") %>% # Divide entre palavra 1 e palavra 2
  filter(!word1 %in% as.vector(t(stop_words$word))) %>%
  filter(!word2 %in% as.vector(t(stop_words$word))) %>% # Filtra as palavras a partir das stop_words
  unite(bigram,word1,word2,sep = " ") %>%
  count(bigram, sort = TRUE)

bigrams %>%
  mutate(bigram = reorder(bigram,n)) %>%
  head(20) %>%
  ggplot(aes(bigram,n,fill=factor(bigram)))+
  scale_fill_manual(values = mycolors)+
  geom_col()+
  xlab(NULL)+
  coord_flip()
```

Bigramas basicamente com citações à familia Bolsonaro. Interessante ver que mesmo usando a palavra-chave Bolsonaro, o segundo bigrama que mais se repetiu foi "Lula Livre".


```{r echo=FALSE} 

# CREATE TRIGAMS
trigrams <- tweets %>%
  unnest_tokens(trigrams, text, token = "ngrams", n = 3) %>% # Separa as palavras de 2 em 2
  separate(trigrams,c("word1","word2", "word3"),sep = " ") %>% # Divide entre palavra 1 e palavra 2
  filter(!word1 %in% as.vector(t(stop_words$word))) %>%
  #filter(!word2 %in% as.vector(t(stopwords_pt$word))) %>% # Filtra as palavras a partir das stop_words
  filter(!word3 %in% as.vector(t(stop_words$word))) %>%
  unite(trigrams,word1,word2, word3,sep = " ") %>%
  count(trigrams, sort = TRUE)

trigrams %>%
  mutate(trigrams = reorder(trigrams,n)) %>%
  head(9) %>%
  ggplot(aes(trigrams,n,fill=factor(trigrams)))+
  scale_fill_manual(values = mycolors) +
  geom_col()+
  xlab(NULL)+
  coord_flip()
```

Nos trigramas, mais algumas citações e alguns comentários sobre a decisão do STF


```{r echo=FALSE}
quadrams <- tweets %>%
  unnest_tokens(quadrams, text, token = "ngrams", n = 4) %>% # Separa as palavras de 2 em 2
  separate(quadrams,c("word1","word2", "word3", "word4"),sep = " ") %>% # Divide entre palavra 1 e palavra 2
  filter(!word1 %in% as.vector(t(stop_words$word))) %>%
  #filter(!word2 %in% as.vector(t(stopwords_pt$word))) %>% # Filtra as palavras a partir das stop_words
  filter(!word4 %in% as.vector(t(stop_words$word))) %>%
  unite(quadrams,word1,word2, word3, word4,sep = " ") %>%
  count(quadrams, sort = TRUE)

quadrams %>%
  mutate(quadrams = reorder(quadrams,n)) %>%
  head(9) %>%
  ggplot(aes(quadrams,n,fill=factor(quadrams)))+
  scale_fill_manual(values = mycolors) +
  geom_col()+
  xlab(NULL)+
  coord_flip()

```


Nos quadrigramas podemos verificar alguns comentários de sobre a reporter que chamou o atual presidente de ex presidente.


```{r echo=FALSE}
affin <- tokens_tweets %>%
  inner_join(affin_pt) %>%
  count(index=linenumber %/% 25, sentiment) %>%
  spread(sentiment,n,fill=0) %>%
  mutate(sentiment = positivo - negativo) #%>%

# Cria um gráfico
# ggplot(affin, aes(index,sentiment))+
  # geom_col(show.legend = TRUE)

positivo <- sum(affin['positivo'])
negativo <- sum(affin['negativo'])



sentimentos <- c("Positivo", "Negativo")
sentimentos_count <- c(sum(affin['positivo']), sum(affin['negativo']))
dados <- data.frame(sentimentos, sentimentos_count, stringsAsFactors = FALSE)

ggplot(data = dados, aes(x = sentimentos)) +
  ylab("Qtd. Palavras") +
  geom_col(aes(y = sentimentos_count), position = "dodge",fill = "grey50", colour = "black")
```

Pesquisa realizada com a palavra-chave bolsonaro retornou dados muito mais negativos, o que acredito já era de se esperar pois já que várias pessoas eram contra a decisão.


### Twites Gerais

E se realizassemos uma pesquisa sem usar nenhuma palavra-chave? Será que a maioria dos usuários estão pensando em politica ou não?

Vamos descobrir! 

```{r fig.keep='all', message=FALSE, warning=FALSE, include=FALSE, paged.print=FALSE, results='hide'}
knitr::opts_chunk$set(tidy = TRUE, tidy.opts = list(comment = FALSE))
m <- mongo(url = "mongodb://172.18.0.3/", options = ssl_options(weak_cert_validation = T), db="twitter", collection="tweets_geral")

#Getting tweets from database and saving on R variable
tweets <- m$find(
  fields = '{"_id": false, "text": true}',
)

patternRegexUrl <- 'https?:\\/\\/(www\\.)?[-a-zA-Z0-9@:%._\\+~#=]{1,256}\\.[a-zA-Z0-9()]{1,6}\\b([-a-zA-Z0-9()@:%_\\+.~#?&//=]*)'
tweets$text <- str_replace(tweets$text, patternRegexUrl, "") 
patternRegexKs <- 'k{2,}'
tweets$text <- str_replace(tweets$text, patternRegexKs, "")


# Removing @users reference from text
tweets$text <- str_replace_all(tweets$text,  "@\\w+", "") %>%
              removePunctuation() %>% # REMOVE PONTUAÇÃO
              stripWhitespace() # REMOVE ESPAÇOS EM BRANCO E PALAVRAS ACENTUADAS

# Removing emojis
# tweets$text <- gsub("[^\x01-\x7F]", "", tweets$text)

# SEPARA POR PARLAVRA E REMOVE AS STOPWORDS
tokens_tweets <- tweets %>%
  mutate(linenumber = row_number()) %>%
  unnest_tokens(word, text) %>%
  anti_join(stop_words)

tokens_tweets$word = str_replace_all(tokens_tweets$word, " ", "")
  
# CONTA AS PALAVRAS
tokens_tweets_count <- tokens_tweets %>%
  filter(word != "bolsonaro") %>%
  count(word, sort = TRUE)

#Aumenta a paleta de cores
nb.cols <- 20
mycolors <- colorRampPalette(brewer.pal(8, "Set2"))(nb.cols)
```
```{r echo= FALSE}

# Cria um Wordcloud
#png("graficos/wordcloud.png", width = 400, height = 400)
wordcloud(tokens_tweets_count$word, 
          tokens_tweets_count$n, 
          max.words =50, 
          scale = c(3,.2),
          rot.per=0.1,
          color=mycolors)
#dev.off()

# Plotando palavras mais usadas
tokens_tweets_count %>%
  mutate(word = reorder(word,n)) %>%
  head(20) %>%
  ggplot(aes(word,n,fill=factor(word)))+
  scale_fill_manual(values = mycolors)+
  geom_col()+
  xlab("Palavras")+
  ylab("Quantidade") +
  coord_flip()


```

Bom, via de regra ao que parece as pessoas não estavam preocupadas com o ocorrido a não ser uma citação ao STF no worldcloud.
  
